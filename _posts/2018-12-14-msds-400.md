---
layout: post
mathjax: true
comments: true
title: Northwestern Data Science Series Part 1
---
## Math For Data Scientists
Hello, and thanks for reading!  This is the first part in a 12 part data science series.  This series is intended to chronicle my journey through the Data Science graduate program at Northwestern University.

Math for Data Scientists (MSDS-400) was the first class I completed in my Data Science graduate program.  The class was designed as a primer/refresher for foundational mathematical concepts used with various modeling techniques.

The core objectives for the class were:  
1. Apply linear programming methods to real world models.
2. Analyze and interpret mathematical models.
3. Create graphs and trees to model real world problems.
4. Calculate and analyze derivatives and integrals of real world models.
5. Evaluate and interpret probabilistic models.

The majority of the class was spent working on linear algebra and calculus.  We took a brief break between these two areas to study algorithms and graph theory.  I'll go into more detail on each of these pieces throughout this article.  Let's dive right in!

### Apply Linear Programming Methods to Real World Models

#### Least Squares Line
The course started out with a relatively basic review of linear functions.  We calculated slopes of lines and did some break-even analysis of linear functions.  We quickly moved on to calculating the least squares line.  The goal of the least squares line is to minimize the sum of the squares of the vertical distances from given data points (on a scatterplot) to the line itself.  Now what does this actually mean?  If you've ever worked in excel and chosen to fit a linear trend line to a scatterplot you've created, then you're already ahead of the game. We're essentially trying to find the line of best fit for a set of data points.  

Let's review a brief example.  The equation for the least squares line $Y=mx+b$ that gives the best fit to the data points $(x_{1},y_{1}), (x_{2},y_{2}),....(x_{n},y_{n})$ has slope $m$ and $y$-intercept $b$ is given by:  

$$
m=\frac{n(\sum xy)-(\sum x)(\sum y)}{n(\sum x^{2})-(\sum x)^{2}}
$$

$$
b=\frac{(\sum y)-m(\sum x)}{n}
$$

This mathematical notation can best be visualized with a small set of data points. We'll use the following table to calculate the least squares line.

| $x$ | $y$ | $xy$ | $x^2$ | $y^2$ |
|:---:|:---:|:----:|:-----:|:-----:|
| 20  | 71.2 | 1424 | 400 | 5069.44 |
|30   | 80.5  | 2415  | 900  |6480.25   |
|40   |73.4   |2936   |1600   |5387.56   |
|50   |60.3   |3015   |2500   |3636.09   |
|60   |52.1   |3126   |3600   |2714.41   |
|70   |56.2   |39.34   |4900   |3158.44   |
|80   |46.5   |3720   |6400   |2162.25   |
|90   |36.9   |3321   |8100   |1361.61   |
|100   |34.0   |3400   |10000   |1156.00   |
|110   |39.1   |4301   |12100   |1528.81   |
|$\sum x = 650$   |$\sum y = 550.2$   |$\sum xy = 31,592$   |$\sum x^2 = 50,500$   |$\sum y^2 = 32,654.86$   |

If we plug the values from the table into our equation we get the following:

$$
m=\frac{10(31,592)-(650)(550.2)}{10(50,500)-(650)^2}
$$

$$
m=-0.506
$$

$$
b=\frac{550.2-(-0.506)(650)}{10}
$$

$$
b=87.9
$$

We now have the components for our least squares line represented by:

$$
Y=-0.506x+87.9
$$

This forms the basis for a very simple method of prediction.  If we are given values for $x$ we can predict $y$. Without knowing how well this line fits our data, we can't be sure our predictions will be accurate. To help us with this problem we turn to the correlation coefficient. The correlation coefficient is denoted by $r$ and is calculated using the following formula:

$$
r=\frac{n(\sum xy)-(\sum x)(\sum y)}{\sqrt{n(\sum x^2)-(\sum x)^2}\cdot \sqrt{n(\sum y^2)-(\sum y)^2}}
$$
