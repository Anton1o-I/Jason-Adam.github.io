---
layout: post
mathjax: true
comments: true
title: Northwestern Data Science Series Part 1
---
## Math For Data Scientists
Hello, and thanks for reading!  This is the first part in a 12 part data science series.  This series is intended to chronicle my journey through the Data Science graduate program at Northwestern University.

Math for Data Scientists (MSDS-400) was the first class I completed in my Data Science graduate program.  The class was designed as a primer/refresher for foundational mathematical concepts used with various modeling techniques.

The book used for the class was

![](/images/msds-400-textbook-cover.png)

*ISBN 978-0-321-97940-7*

The core objectives for the class were:  
1. Apply linear programming methods to real world models.
2. Analyze and interpret mathematical models.
3. Create graphs and trees to model real world problems.
4. Calculate and analyze derivatives and integrals of real world models.
5. Evaluate and interpret probabilistic models.

The majority of the class was spent working on linear algebra and calculus.  We took a brief break between these two areas to study algorithms and graph theory.  I'll go into more detail on each of these pieces throughout this post. In order to avoid a ridiculously long blog post, I'm going to highlight components of each objective that I found to be most helpful and foundational. Many of these topics could constitute an entire post of their own, and I may circle back to some of them with more in-depth reviews at a later time. Let's dive in!

### Apply Linear Programming Methods to Real World Models

#### Least Squares Line
We'll start out with a relatively basic review of linear functions.  One of the most basic predictive methods is the least squares line. The goal of the least squares line is to minimize the sum of the squares of the vertical distances from given data points (on a scatterplot) to the line itself.  Now what does this actually mean?  If you've ever worked in excel and chosen to fit a linear trend line to a scatterplot you've created, then you're already ahead of the game. We're essentially trying to find the line of best fit for a set of data points.  

Let's review a brief example.  The equation for the least squares line $Y=mx+b$ that gives the best fit to the data points $(x_{1},y_{1}), (x_{2},y_{2}),....(x_{n},y_{n})$ has slope $m$ and $y$-intercept $b$ is given by:  

$$
m=\frac{n(\sum xy)-(\sum x)(\sum y)}{n(\sum x^{2})-(\sum x)^{2}}
$$

$$
b=\frac{(\sum y)-m(\sum x)}{n}
$$

This mathematical notation can best be visualized with a small set of data points. We'll use the following table to calculate the least squares line.

| $x$ | $y$ | $xy$ | $x^2$ | $y^2$ |
|:---:|:---:|:----:|:-----:|:-----:|
| $20$  | $71.2$ | $1,424$ | $400$ | $5,069.44$ |
|$30$   | $80.5$  | $2,415$  | $900$  |$6,480.25$   |
|$40$   |$73.4$   |$2,936$   |$1,600$   |$5,387.56$   |
|$50$   |$60.3$   |$3,015$   |$2,500$   |$3,636.09$   |
|$60$   |$52.1$   |$3,126$   |$3,600$   |$2,714.41$   |
|$70$   |$56.2$   |$3,934$   |$4,900$   |$3,158.44$   |
|$80$   |$46.5$   |$3,720$   |$6,400$   |$2,162.25$   |
|$90$   |$36.9$   |$3,321$   |$8,100$   |$1,361.61$   |
|$100$   |$34.0$   |$3,400$   |$10,000$   |$1,156.00$   |
|$110$   |$39.1$   |$4,301$   |$12,100$   |$1,528.81$   |
|$\sum x = 650$   |$\sum y = 550.2$   |$\sum xy = 31,592$   |$\sum x^2 = 50,500$   |$\sum y^2 = 32,654.86$   |

If we plug the values from the table into our equation we get the following:

$$
m=\frac{10(31,592)-(650)(550.2)}{10(50,500)-(650)^2}
$$

$$
m=-0.506
$$

$$
b=\frac{550.2-(-0.506)(650)}{10}
$$

$$
b=87.9
$$

We now have the components for our least squares line represented by:

$$
Y=-0.506x+87.9
$$

This forms the basis for a very simple method of prediction.  If we are given values for $x$ we can predict $y$. Without knowing how well this line fits our data, we can't be sure our predictions will be accurate. To help us with this problem we turn to the correlation coefficient. The correlation coefficient is denoted by $r$ and is calculated using the following formula:

$$
r=\frac{n(\sum xy)-(\sum x)(\sum y)}{\sqrt{n(\sum x^2)-(\sum x)^2}\cdot \sqrt{n(\sum y^2)-(\sum y)^2}}
$$

The correlation coefficient measures the strength of the linear relationship between two variables and was developed by Karl Pearson (1857 - 1936). The coefficient always lies between $-1$ and $1$. Values of $1$ or $-1$ indicate that the data points lie exactly on the least squares line. A coefficient of $1$ represents a positive slope while $-1$ represents a negative slope.  If $r=0$, there is no linear correlation between the data points.  Continuing with our sample data above we can calculate the correlation coefficient:

$$
r=-0.936
$$

$$
r^2=0.876
$$

Our $r$ value indicates a strong negative correlation between our variables.  We observe the $r^2$ value of $0.876$ which represents the variation in $y$ that is explained by the linear relationship between $x$ and $y$. This means that approximately $87.6\%$ of the variation in $y$ is explained by the linear relationship. Even though we have a strong linear correlation, this does not always imply causation.  There are further techniques that can be applied that are outside the scope of this post.

#### Systems of Linear Equations
Moving on from the least squares line, we expand into solving systems of linear equations. Many mathematical models require finding the solution of two or more equations, and the solution must satisfy all of the equations of the model. This forms the basis for a **system of equations**.  In this section we'll review two different methods for solving these systems:
1. Echelon Method
2. Gauss-Jordan Method

##### Echelon Method
The Echelon Method is a systematic approach for solving systems of equations using the *three transformations of a system*. The three transformations are comprised of algebraic properties and can be defined as follows:
1. Exchanging any two equations
2. Multiplying both sides of an equation by any nonzero real number
3. Replacing any equation by a nonzero multiple of that equation plus a nonzero multiple of any other equation

The Echelon method can be thought of as an elimination method.  The elimination method allows for any variable to be eliminated. Once we're able to eliminate a variable, we can use *back substitution* to calculate the remaining variable. Let's look at a simplified example of this:

Let's solve this system of equations:

$$
3x+10y=115
$$

$$
11x+4y=95
$$

We can use the transformation rules to eliminate the $x$ variable and isolate the $y$ variable. We will use $R_1$ to denote the first equation and $R_2$ to denote the second. The first equation remains unchanged, and the second is transformed to:

$$
11R_1+(-3)R_2\rightarrow R_2
$$

This results in the transformation of $R_2$ to:

$$
98y=980
$$

$$
y=10
$$

We can now substitute 10 for $y$ in our first equation and calculate for $x$:

$$
x=5
$$

We can plot the same lines using some simple python syntax with matplotlib to check if our elimination calculated the correct point of intersection:

```python
import matplotlib.pyplot as plt
import numpy as np

x = np.array([0,1,2,3,4,5,6,7,8,9,10])

r1 = (-3/10)*x+(115/10)
r2 = (-11/4)*x+(95/4)

plt.plot(r1, label='3x + 10y = 115')
plt.plot(r2, label='11x + 4y = 95')
plt.plot(5, 10, 'ro', label='Solution')
plt.legend(loc='lower left')
plt.title('Echelon Example')
plt.show()
```

![](/jupyter-notebooks/echelon-example.png)

We can see from the graph that we calculated our solution correctly. Normally it's helpful to visualize the equations when possible to see if your algebraic solution makes sense.

##### Gauss-Jordan Method

Our second method for dealing with systems of equations is the Gauss-Jordan method. The Gauss-Jordan (GJ) method is an extension of the Echelon method of solving systems. Before we can apply the GJ method, we must convert the equations into the proper form: the terms with variables should be on the left and the constants on the right in each equation, with the variables in the same order in each equation. The system is then written as an *augmented matrix*.  An augmented matrix consists of just the coefficients and constants from a system of equations with the farthest right column separated from the rest of the matrix by a vertical line \[1\].

Here is an example of a simple system of equations:

$$
x+5z=-6+y
$$

$$
3x+3y=10+z
$$

$$
x+3y+2z=5
$$

We first need to rewrite the system in the proper form described above. The resulting system becomes:

$$
x-y+5z=-6
$$

$$
3x+3y-z=10
$$

$$
x+3y+2z=5
$$

Now that we have our system in the correct form we can create our augmented matrix:

$$
\left[
\begin{array}{ccc|c}
1 & -1 & 5 & -6 \\
3 & 3 & -1 & 10 \\
1 & 3 & 2 & 5
\end{array}
\right]
$$

The goal when using the augmented matrix is to transform the rows so that the final matrix consists of $0$'s above and below the diagonal of $1$'s on the left of the vertical bar. Manually transforming the matrix can become a long and arduous process. Personally, I'm not a huge fan of reducing the matrix by hand as it's much more prone to error. For the sake of this post, I will not go through each iterative row transformation but will instead show how to solve the system through the use of python.

Conveniently, we can use the powerful library **numpy** to perform our calculations. If you're not familiar with the numpy library, a quick google search will provide you with an abundance of resources.

We can start by building our augmented matrix using numpy arrays:

```python
import numpy as np

# Create matrix using numpy arrays
# We split the augmented matrix into left-hand side (lhs) and right-hand side (rhs)
lhs = np.array([[1,-1,5], [3,3,-1], [1,3,2]])
rhs = np.array([-6,10,5])

# Solve the system
solution = np.linalg.solve(lhs, rhs)

print('x = {:.0f}'.format(solution[0]))
print('y = {:.0f}'.format(solution[1]))
print('z = {:.0f}'.format(solution[2]))
```

The output produced by our code is:

$$
x=1
$$

$$
y=2
$$

$$
z=-1
$$

If we substitute these values back into our original equations, we can see that we've correctly solved the system.

In addition to these two methods we also covered matrix addition, subtraction, & multiplication. We also reviewed how to calculate the inverse of a matrix. In order to keep this post relatively concise, I'm not going to cover each of those topics. Perhaps they will be a good topic for their own future post!

##### The Simplex Method

The final section I'm going to cover regarding **Linear Programming** is the *Simplex Method*. The simplex method

### References

\[1\] Lial, M. L., Greenwell, R. N., & Ritchey, N. P. (2016). Finite mathematics and calculus with  applications (10th ed.). Boston: Pearson.
