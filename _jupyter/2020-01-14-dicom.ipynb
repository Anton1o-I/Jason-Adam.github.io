{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post  \n",
    "mathjax: true  \n",
    "comments: true  \n",
    "title: DICOM Processing  \n",
    "tags: [Computer Vision]  \n",
    "---  \n",
    "What is the DICOM file format and how does it relate to computer vision?  \n",
    "\n",
    "A little while ago I decided to undertake a computer vision project to detect abnormalities in brain CT scans by utilizing 3-dimensional [convolutional neural networks](https://jason-adam.github.io/convolutions/). Admittedly, this was a little more ambitious than I had time for, but it was a great learnring experience, and it forced me to work on my pre-processing skills due to the nature of the file formats that certain medical images come in. This leads me to the DICOM file format for medical imaging.   \n",
    "## What is a DICOM file?  \n",
    "\n",
    "[DICOM](https://en.wikipedia.org/wiki/DICOM) stands *Digital Imaging and Communications in Medicine*, and is considered the standard for communication and management of medical imaging information and related data. The most common usage for DICOM is to store and transmit medical images.  \n",
    "\n",
    "Now that we have a very cursory understanding of what DICOM is and what it's used for, lets start on our pre-processing pipeline for these medical images.  \n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The blackcellmagic extension is already loaded. To reload it, use:\n",
      "  %reload_ext blackcellmagic\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pydicom\n",
    "import scipy.ndimage\n",
    "\n",
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL VARIABLES\n",
    "INPUT_FOLDER = \"../imgs/2020-01-12-dicom/sample_dicom/Unknown Study/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the Data  \n",
    "Before jumping into the code, I want to take a quick step back. The original study where I obtained the data can be found [here](http://headctstudy.qure.ai/#dataset). The original study predicted multiple labels for the patient (i.e. intracranial hemorrhage, fracture, midline shifts, etc.), but for the simplification of my project, I selected the abnormality affecting the most scans in the sample dataset I downloaded (Intracranial Hemorrhage).  \n",
    "\n",
    "Below is a simple way to download one zip folder for a patient with their scans. If you want to download all the zip folders, I recommend giving yourself some time, as there are almost 500 zip files, each containing a full set (sometimes multiple sets) of scans for a patient. The total size of all patients downloaded and unzipped is roughly 40-50GB."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!wget https://s3.ap-south-1.amazonaws.com/qure.headct.study/CQ500-CT-0.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the scans have downloaded, we can use the following code to see which size of scans (number of slices is the most prominent (they come in various slices). The function simply loops through our scan folders and counts up the amount of slices into a list of tuples using the `Counter` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(239, 3)]\n"
     ]
    }
   ],
   "source": [
    "def most_common_slice_count() -> List[Tuple]:\n",
    "    slice_counts: list = []\n",
    "    list_scans = os.listdir(INPUT_FOLDER)\n",
    "\n",
    "    for scan_folders in list_scans:\n",
    "        slice_path = os.path.join(INPUT_FOLDER, scan_folders + \"/\")\n",
    "        slices = os.listdir(slice_path)\n",
    "        slice_counts.append(len(slices))\n",
    "\n",
    "    counts = Counter(slice_counts)\n",
    "    return counts.most_common(1)\n",
    "\n",
    "\n",
    "print(most_common_slice_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Pipeline  \n",
    "For this example, we only have one patient, but their folder contains three sets of scans all consisting of 239 slices. The next function using the `pydicom` library which was created for dealing with DICOM image files. The code takes in an argument for the size of slices to retrieve (239 in our case). This is a generator function which means that it returns an iterator. If you're unfamiliar with the concept, there is a great guide [here](https://realpython.com/introduction-to-python-generators/). The code reads in all the scans using the `pydicom.read_file` function and loads them to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaded_scans(base_path: str, slice_count: int):\n",
    "    list_scans = os.listdir(INPUT_FOLDER)\n",
    "\n",
    "    for scan_folders in list_scans:\n",
    "        slice_path = os.path.join(INPUT_FOLDER, scan_folders + \"/\")\n",
    "        slices = os.listdir(slice_path)\n",
    "        if len(slices) != slice_count:\n",
    "            continue\n",
    "        else:\n",
    "            slices.sort()\n",
    "            yield list(\n",
    "                [pydicom.read_file(os.path.join(slice_path, s)) for s in slices]\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test to see if our generator function works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239\n",
      "239\n",
      "239\n"
     ]
    }
   ],
   "source": [
    "test_scans = get_loaded_scans(INPUT_FOLDER, slice_count=239)\n",
    "\n",
    "for scan in test_scans:\n",
    "    print(len(scan))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we did yield three sets of scans of length 239! It looks like our generator function is working as intended. Before we look at the next function, there is some additional information regarding CT scans that is relevant. The pixels in a CT scan are represented in [Hounsfield Units](https://en.wikipedia.org/wiki/Hounsfield_scale). The Hounsfield scale is a quantitative scale for describing radiodensity, and the value ranges represent different types of tissue. Below is an image of the value ranges for context.  \n",
    "\n",
    "![](../imgs/2020-01-12-dicom/hounsfield_scale.jpg)  \n",
    "\n",
    "Prior to looking at any scans, we can guess that we'll see several of these ranges, including bone, grey matter, etc.  \n",
    "\n",
    "Now that we have our intial generator function setup, I'm going to show what's actually in a DICOM file prior to skipping to extracting the pixels. Let's read in one for our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0008, 0005) Specific Character Set              CS: 'ISO_IR 100'\n",
      "(0008, 0008) Image Type                          CS: ['ORIGINAL', 'PRIMARY', 'AXIAL']\n",
      "(0008, 0012) Instance Creation Date              DA: ''\n",
      "(0008, 0013) Instance Creation Time              TM: ''\n",
      "(0008, 0016) SOP Class UID                       UI: CT Image Storage\n",
      "(0008, 0018) SOP Instance UID                    UI: 1.2.276.0.7230010.3.1.4.296485376.1.1521714308.2032317\n",
      "(0008, 0020) Study Date                          DA: ''\n",
      "(0008, 0023) Content Date                        DA: ''\n",
      "(0008, 0030) Study Time                          TM: ''\n",
      "(0008, 0033) Content Time                        TM: ''\n",
      "(0008, 0050) Accession Number                    SH: ''\n",
      "(0008, 0060) Modality                            CS: 'CT'\n",
      "(0008, 0070) Manufacturer                        LO: ''\n",
      "(0008, 0090) Referring Physician's Name          PN: ''\n",
      "(0008, 103e) Series Description                  LO: '4cc sec 150cc D3D on'\n",
      "(0008, 1090) Manufacturer's Model Name           LO: ''\n",
      "(0008, 9215)  Derivation Code Sequence   1 item(s) ---- \n",
      "   (0008, 0100) Code Value                          SH: '121327'\n",
      "   (0008, 0102) Coding Scheme Designator            SH: 'DCM'\n",
      "   (0008, 0104) Code Meaning                        LO: 'Full fidelity image'\n",
      "   ---------\n",
      "(0010, 0010) Patient's Name                      PN: 'CQ500-CT-0'\n",
      "(0010, 0020) Patient ID                          LO: 'CQ500-CT-0'\n",
      "(0010, 0030) Patient's Birth Date                DA: ''\n",
      "(0010, 0040) Patient's Sex                       CS: ''\n",
      "(0012, 0062) Patient Identity Removed            CS: 'YES'\n",
      "(0018, 0022) Scan Options                        CS: 'HELICAL MODE'\n",
      "(0018, 0050) Slice Thickness                     DS: \"0.625\"\n",
      "(0018, 0060) KVP                                 DS: \"120.0\"\n",
      "(0018, 0088) Spacing Between Slices              DS: \"0.625\"\n",
      "(0018, 0090) Data Collection Diameter            DS: \"250.0\"\n",
      "(0018, 1020) Software Versions                   LO: 'qin.20'\n",
      "(0018, 1100) Reconstruction Diameter             DS: \"227.0\"\n",
      "(0018, 1110) Distance Source to Detector         DS: \"949.075\"\n",
      "(0018, 1111) Distance Source to Patient          DS: \"541.0\"\n",
      "(0018, 1120) Gantry/Detector Tilt                DS: \"8.5\"\n",
      "(0018, 1130) Table Height                        DS: \"174.0\"\n",
      "(0018, 1140) Rotation Direction                  CS: 'CW'\n",
      "(0018, 1150) Exposure Time                       IS: \"1296\"\n",
      "(0018, 1151) X-Ray Tube Current                  IS: \"330\"\n",
      "(0018, 1152) Exposure                            IS: \"21\"\n",
      "(0018, 1160) Filter Type                         SH: 'HEAD FILTER'\n",
      "(0018, 1170) Generator Power                     IS: \"39600\"\n",
      "(0018, 1190) Focal Spot(s)                       DS: \"1.2\"\n",
      "(0018, 1210) Convolution Kernel                  SH: 'STANDARD'\n",
      "(0018, 5100) Patient Position                    CS: 'HFS'\n",
      "(0018, 9305) Revolution Time                     FD: 0.6\n",
      "(0018, 9306) Single Collimation Width            FD: 0.625\n",
      "(0018, 9307) Total Collimation Width             FD: 10.0\n",
      "(0018, 9309) Table Speed                         FD: 9.479120572408041\n",
      "(0018, 9310) Table Feed per Rotation             FD: 5.687472343444824\n",
      "(0018, 9311) Spiral Pitch Factor                 FD: 0.5687472343444824\n",
      "(0020, 000d) Study Instance UID                  UI: 1.2.276.0.7230010.3.1.2.296485376.1.1521714307.2031999\n",
      "(0020, 000e) Series Instance UID                 UI: 1.2.276.0.7230010.3.1.3.296485376.1.1521714308.2032152\n",
      "(0020, 0010) Study ID                            SH: ''\n",
      "(0020, 0011) Series Number                       IS: \"6\"\n",
      "(0020, 0012) Acquisition Number                  IS: \"1\"\n",
      "(0020, 0013) Instance Number                     IS: \"170\"\n",
      "(0020, 0032) Image Position (Patient)            DS: [-113.500, -106.053, 92.725]\n",
      "(0020, 0037) Image Orientation (Patient)         DS: [1.000000, 0.000000, 0.000000, 0.000000, 0.989016, -0.147809]\n",
      "(0020, 1040) Position Reference Indicator        LO: 'OM'\n",
      "(0020, 1041) Slice Location                      DS: \"76.875\"\n",
      "(0028, 0002) Samples per Pixel                   US: 1\n",
      "(0028, 0004) Photometric Interpretation          CS: 'MONOCHROME2'\n",
      "(0028, 0010) Rows                                US: 512\n",
      "(0028, 0011) Columns                             US: 512\n",
      "(0028, 0030) Pixel Spacing                       DS: [0.443359, 0.443359]\n",
      "(0028, 0100) Bits Allocated                      US: 16\n",
      "(0028, 0101) Bits Stored                         US: 16\n",
      "(0028, 0102) High Bit                            US: 15\n",
      "(0028, 0103) Pixel Representation                US: 1\n",
      "(0028, 0120) Pixel Padding Value                 SS: -2000\n",
      "(0028, 1050) Window Center                       DS: \"30.0\"\n",
      "(0028, 1051) Window Width                        DS: \"300.0\"\n",
      "(0028, 1052) Rescale Intercept                   DS: \"-1024.0\"\n",
      "(0028, 1053) Rescale Slope                       DS: \"1.0\"\n",
      "(0028, 1054) Rescale Type                        LO: 'HU'\n",
      "(0040, 0009) Scheduled Procedure Step ID         SH: 'ANON'\n",
      "(7fe0, 0010) Pixel Data                          OB: Array of 175622 elements\n"
     ]
    }
   ],
   "source": [
    "sample_slice = pydicom.read_file(\n",
    "    \"../imgs/2020-01-12-dicom/sample_dicom/Unknown Study/CT 4cc sec 150cc D3D on/CT000082.dcm\"\n",
    ")\n",
    "\n",
    "print(sample_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the DICOM slice contains very rich metadata about the patient and the slice itself. The final element in the slice contains an array of elements. These are our pixels.  \n",
    "\n",
    "The following function was adapted and modified from [here](https://www.kaggle.com/saptarc/processing-lungs-ct-scans). It stacks all scans into a 3D numpy array, adjusts pixels based on a padding value, and converts to hounsfield units. The results is a transformed 3D numpy array. Since this function accepts a list (an iterator), we can pass the output of our previous function to it. This function also is a generator (it returns an iterator). We can begin to see why they are so powerful for preprocessing pipelines. The resultant iterator is only loaded into memory one at a time instead of the entire batch. This makes these pipelines extremely fast and memory efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixels_hu(slices: list):\n",
    "    for scans in slices:\n",
    "        try:\n",
    "            patient_id = str(scans[0].PatientName)\n",
    "            image = np.stack([s.pixel_array for s in scans])\n",
    "            image = image.astype(np.float)\n",
    "\n",
    "            # Set outside-of-scan pixels to 0\n",
    "            image[image == -2000] = 0\n",
    "\n",
    "            # Convert to Hounsfield units (HU)\n",
    "            for slice_number in range(len(scans)):\n",
    "\n",
    "                intercept = scans[slice_number].RescaleIntercept\n",
    "                slope = scans[slice_number].RescaleSlope\n",
    "\n",
    "                if slope != 1:\n",
    "                    image[slice_number] = slope * image[slice_number]\n",
    "\n",
    "                image[slice_number] += np.float(intercept)\n",
    "\n",
    "            yield np.array(image, dtype=np.float), patient_id\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final function in the pipleine is a normalization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_stacks(image, min_bound=-1000.0, max_bound=2000.0):\n",
    "    image = (image - min_bound) / (max_bound - min_bound)\n",
    "    image[image > 1] = 1\n",
    "    image[image < 0] = 0\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a simple sample of our total pipeline that was in the main function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scans = get_loaded_scans(INPUT_FOLDER, slice_count=239)\n",
    "scan_array = get_pixels_hu(test_scans)\n",
    "\n",
    "for i, j in scan_array:\n",
    "    normalized = scipy.ndimage.interpolation.zoom(i, (1, 0.50, 0.50), mode=\"nearest\")\n",
    "    normalized = normalize_stacks(normalized)\n",
    "    normalized = normalized - np.mean(normalized)\n",
    "    print(normalized.shape, j)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jason-Adam.github.io",
   "language": "python",
   "name": "jason-adam.github.io"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
